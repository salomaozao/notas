Então, temos as hipóteses:
$$
\begin{eqnarray}
          \left\{\begin{array}{l}
      H_{0}: \beta_{0} = \beta_{00} \\
      H_{1}: \beta_{0} \neq \beta_{00}
      \end{array}\right.
      ~~ \mbox{   e   } ~~
      \left\{\begin{array}{l}
      H_{0}: \beta_{1} = \beta_{10} \\
      H_{1}: \beta_{1} \neq \beta_{10}
      \end{array}\right.
       \nonumber,
  \end{eqnarray}
$$
$$
\begin{eqnarray}
      \frac{\hat{\beta}_{0}-\beta_{00}}{\sqrt{\hat{\sigma}^{2}\left(\frac{1}{n}+\frac{\bar{x}^2}{S_{xx}}\right)}} \sim t_{n-2}
      ~~ \mbox{   e   } ~~
      \frac{\hat{\beta}_{1}-\beta_{10}}{\sqrt{\frac{\hat{\sigma}^{2}}{S_{xx}}}} \sim t_{n-2}.   \nonumber
  \end{eqnarray}
$$
em que $\beta_00$ e $\beta_10$ correspondem à valores arbitrários (geralmente zero) para $\beta_0$ e $\beta_1$ .Lembrando que $x_i$ é a covariável

Para que a **regressão linear seja significativa**, precisamos que beta 1 for diferente de 0, para que haja contriubuição de $x_i$, dado  $y_i =\beta_0+\beta_1 x_i + e$ se torne $y_i = \beta$

se $\beta_1 = 0 \Rightarrow \hat{y} \sim \overline{y}$

$$
\begin{eqnarray}
      SQTotal = SQReg + SQRes. \nonumber
  \end{eqnarray}
$$
$$
\begin{eqnarray}
        \sum_{i=1}^{n}(y_{i}-\bar{y})^2 = \sum_{i=1}^{n}(\hat{y}_{i}-\bar{y})^2 + \sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^2. \nonumber
    \end{eqnarray}
$$

então a equação acima tem a variância dependente da *soma dos quadrados dos resíduos*, onde $E(QMRes) = E\left(\frac{SQRes}{n-2}\right) = \sigma^2. \nonumber$, logo, $\hat{\sigma}^2=QMRes$ é um estimador não viciado para $\sigma^2$

para testar a significância da regressão:
$$
\begin{eqnarray}
      {
                 \left\{\begin{array}{lcc}
      H_{0}: \beta_{1} = 0 \\
      H_{1}: \beta_{1} \neq 0
      \end{array}\right.
      }    \nonumber
  \end{eqnarray}
$$


## Tabela [[ANOVA]]
![[Inferências Usando Regressão-20240809212936717.webp|828]]
realizamos a ANOVA para avaliar a signific

- **Soma dos Quadrados do Modelo (SSM)**: Mede a variabilidade dos valores de Y que é explicada pelo modelo de regressão.
- **Soma dos Quadrados do Resíduo (SSR)**: Mede a variabilidade dos valores de YYY que não é explicada pelo modelo de regressão.
### Caso de uso:
```
# ajustando o modelo:
mod <- lm(pureza ~ percentual, data = pureza)

# verificando a classe do objeto mod:
class(mod)
```

```
[1] "lm"
```
***
```
# extraíndo um sumário do modelo ajustado:
summary(mod)
```

```
Call:
lm(formula = pureza ~ percentual, data = pureza)

Residuals:
     Min       1Q   Median       3Q      Max 
-1.83029 -0.73334  0.04497  0.69969  1.96809 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   74.283      1.593   46.62  < 2e-16 ***
percentual    14.947      1.317   11.35 1.23e-09 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 1.087 on 18 degrees of freedom
Multiple R-squared:  0.8774,    Adjusted R-squared:  0.8706 
F-statistic: 128.9 on 1 and 18 DF,  p-value: 1.227e-09
```

Podemos concluir o seguinte:
#### Coeficientes:
- **$\beta_0$, Intercepto**
- **$\beta_1$ Percentual (regressão)**
	Ambos tem p-valor de alta significância (**" * * * "**), então podemos concluir que ambos tem [[Efeito]] significativo, logo $\beta_0=0$

#### Componentes:
- **$\hat{\beta}$, Estimate**: valor estimado do coeficiente para cada variável no modelo de regressão. No caso, $\hat{\beta_0}=74.283, \hat{\beta_1}=14.947$
- **$\sqrt{Var(\hat{\beta})}$ , Std. Error**: Variância da Estimativa do Coeficiente
	Podemos inferir, à 99.9% de confiança que o valor real de $\beta_0$ está dentro do IC: $\hat{\beta} \pm t_{\text{crítico}} * \sqrt{Var(\hat{\beta})}$
***
```
> anova(mod)

Analysis of Variance Table

Response: pureza
           Df Sum Sq Mean Sq F value    Pr(>F)    
percentual  1 152.13 152.127  128.86 1.227e-09 ***
Residuals  18  21.25   1.181                      
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```